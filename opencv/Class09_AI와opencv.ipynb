{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31372b8-d348-48f5-b7a8-7d6791c4b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ca597-c5bd-44f7-a751-1f2e4fd56323",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './googlenet/fig/apple2.png'\n",
    "\n",
    "img = cv2.imread(filename)\n",
    "\n",
    "model = './googlenet/bvlc_googlenet.caffemodel'\n",
    "config = './googlenet/deploy.prototxt'\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load fialed')\n",
    "    sys.exit()\n",
    "    \n",
    "# 분류명 불러오기\n",
    "classNames = []\n",
    "with open('./googlenet/classification_classes_ILSVRC2012.txt', 'r') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "# scalefactor \n",
    "# deepleaning 그림 이미지 사이즈는 (244, 244) 사이즈로미리 결정하고, 고정시킨다.\n",
    "# 이미지 설정은 config 파일을 찾아서 확인할 수 밖에 없다.\n",
    "blob = cv2.dnn.blobFromImage(img, 1, (244, 244), (104, 117, 123), swapRB = False)\n",
    "net.setInput(blob)\n",
    "prob = net.forward()\n",
    "\n",
    "print(prob.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a79c1-be90-4ac5-9ddf-39fc15a529bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob는 다루기 쉽게 자른다.\n",
    "out = prob.flatten()\n",
    "classId = np.argmax(out)\n",
    "\n",
    "print(classId)\n",
    "print(classNames[classId])\n",
    "print(out[classId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821f850-b38d-4d03-9fbc-5855a28885bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = out[classId]\n",
    "category = classNames[classId]\n",
    "text = f'{category} ({confidence*100:4.2f} %)'\n",
    "cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_COMPLEX,\n",
    "            1, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9024500-8d34-4e38-a17f-d11392e49518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 슬라이드로 보여주기\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "model = './googlenet/bvlc_googlenet.caffemodel'\n",
    "config = './googlenet/deploy.prototxt'\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load fialed')\n",
    "    sys.exit()\n",
    "    \n",
    "# 분류명 불러오기\n",
    "classNames = []\n",
    "with open('./googlenet/classification_classes_ILSVRC2012.txt', 'r') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "img_lists = glob.glob('./googlenet/fig/sweet/image*.jpg')\n",
    "\n",
    "cv2.namedWindow('scene', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('scene', 488, 488) \n",
    "\n",
    "idx = 0\n",
    "while True:\n",
    "    if len(img_lists) <= 0:\n",
    "        print('images is zeros')\n",
    "        break\n",
    "    \n",
    "    print(idx)\n",
    "    img = cv2.imread(img_lists[idx])\n",
    "    if img is None:\n",
    "        print('image read failed')\n",
    "        break\n",
    "        \n",
    "        \n",
    "    blob = cv2.dnn.blobFromImage(img, 1, (244, 244), (104, 117, 123), swapRB = False)\n",
    "    net.setInput(blob)\n",
    "    prob = net.forward()\n",
    "    out = prob.flatten()\n",
    "    classId = np.argmax(out)\n",
    "    confidence = out[classId]\n",
    "    category = classNames[classId]\n",
    "    text = f'{category} ({confidence*100:4.2f} %)'\n",
    "    cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "    cv2.imshow('scene', img)\n",
    "\n",
    "\n",
    "    if cv2.waitKey(3000) == 27:\n",
    "        break\n",
    "\n",
    "    idx +=1\n",
    "    if idx >= len(img_lists):\n",
    "        idx = 0\n",
    "            \n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13bc328-720e-4375-b5f7-7fe75db36a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# face detection network download \n",
    "# 얼굴 인식\n",
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('./opencv_face_detector/fig/sunglass.png')\n",
    "\n",
    "model = './opencv_face_detector/opencv_face_detector_uint8.pb'\n",
    "config = './opencv_face_detector/opencv_face_detector.pbtxt'\n",
    "\n",
    "face_detect_net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if face_detect_net.empty():\n",
    "    print('net load error')\n",
    "    sys.exit\n",
    "    \n",
    "blob = cv2.dnn.blobFromImage(img, 1, (300, 300), (104, 177, 123),\n",
    "                     swapRB = False)\n",
    "\n",
    "face_detect_net.setInput(blob)\n",
    "\n",
    "out = face_detect_net.forward()\n",
    "\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "# out은 1, 1 200 * 7 배열이 나온다. \n",
    "# 해당 배열은 0, 1번째는 사용하지 않고, 3번째 부터 사용한다.\n",
    "# 2번째는 사물이 얼굴로 분류된 확률이다.\n",
    "# 3, 4, 5, 6열은 (x1, y1), (x2, y2) 좌표이며, 사각형을 만들 수 있는 좌표다.\n",
    "# 값이 확률로 기록되어 확인을 해서 계산을 해줘야 한다.\n",
    "# 앞에 1, 1은 사용하지 않으므로 0으로 제거해준다.\n",
    "detect = out[0, 0, :, :]\n",
    "for i in range(detect.shape[0]):\n",
    "    confidence = detect[i, 2] \n",
    "    \n",
    "    if confidence  > 0.1:  # 얼굴 인식할 box 위치를 뽑아낸다.\n",
    "        x1 = int(detect[i, 3] * w) \n",
    "        y1 = int(detect[i, 4] * h)\n",
    "        x2 = int(detect[i, 5] * w) \n",
    "        y2 = int(detect[i, 6] * h) \n",
    "        \n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        text = 'Face: {}%'.format(round(confidence* 100, 2))\n",
    "        cv2.putText(img, text, (x1, y1 - 4), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                    0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "print(out.shape)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66f0d8-293f-479b-a69f-5ae24ce51980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "#model = './opencv_face_detector/res10_300x300_ssd_iter_140000_fp16.caffemode'\n",
    "model = './opencv_face_detector/opencv_face_detector_uint8.pb'\n",
    "config = './opencv_face_detector/opencv_face_detector.pbtxt'\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('video open failed')\n",
    "    sys.exit()\n",
    "    \n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('net load failed')\n",
    "    sys.exit()\n",
    "    \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print('frame read failed')\n",
    "        break\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1, (300, 300), (104, 177, 123))\n",
    "    net.setInput(blob)\n",
    "    out = net.forward()\n",
    "    \n",
    "    detect = out[0, 0, :, :]\n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    for i in range(detect.shape[0]):\n",
    "        confidence = detect[i, 2]\n",
    "        \n",
    "        if confidence > 0.5:\n",
    "            x1 = int(detect[i, 3] * w) \n",
    "            y1 = int(detect[i, 4] * h)\n",
    "            x2 = int(detect[i, 5] * w) \n",
    "            y2 = int(detect[i, 6] * h) \n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "            text = 'Face: {}%'.format(round(confidence* 100, 2))\n",
    "            cv2.putText(frame, text, (x1, y1 - 4), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(30) == 27:\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38eefdbe-e5fd-4e1b-9b6e-65c7de945527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26167a87-330d-4715-9dcd-e6d01b111e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n",
      "float64\n",
      "float64\n",
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(x_train.dtype)\n",
    "\n",
    "# 영상으로 쓰기 위해서 차원을 1로 스케일링 한다. + float 값으로 변환\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)/255.\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)/255.\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "# one hot encding\n",
    "#y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "print(x_train.dtype)\n",
    "print(x_test.dtype)\n",
    "print(y_train.dtype)\n",
    "print(y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dae4bddd-ad4d-4b3b-9743-abfd1da1239d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               1179776   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, kernel_size  = (3, 3), input_shape= (28, 28, 1), \n",
    "                              activation = 'relu'))\n",
    "model.add(keras.layers.Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size = 2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(128, activation = 'relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16a5d321-4304-425f-804c-ebf6745693af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "210/210 [==============================] - 69s 328ms/step - loss: 0.2857 - accuracy: 0.9135 - val_loss: 0.0759 - val_accuracy: 0.9770\n",
      "Epoch 2/10\n",
      "210/210 [==============================] - 65s 309ms/step - loss: 0.0727 - accuracy: 0.9782 - val_loss: 0.0537 - val_accuracy: 0.9844\n",
      "Epoch 3/10\n",
      "210/210 [==============================] - 65s 308ms/step - loss: 0.0520 - accuracy: 0.9838 - val_loss: 0.0523 - val_accuracy: 0.9837\n",
      "Epoch 4/10\n",
      "210/210 [==============================] - 65s 311ms/step - loss: 0.0363 - accuracy: 0.9880 - val_loss: 0.0471 - val_accuracy: 0.9862\n",
      "Epoch 5/10\n",
      "210/210 [==============================] - 68s 325ms/step - loss: 0.0294 - accuracy: 0.9907 - val_loss: 0.0430 - val_accuracy: 0.9874\n",
      "Epoch 6/10\n",
      "210/210 [==============================] - 66s 316ms/step - loss: 0.0236 - accuracy: 0.9921 - val_loss: 0.0466 - val_accuracy: 0.9873\n",
      "Epoch 7/10\n",
      "210/210 [==============================] - 66s 313ms/step - loss: 0.0198 - accuracy: 0.9930 - val_loss: 0.0593 - val_accuracy: 0.9851\n",
      "Epoch 8/10\n",
      "210/210 [==============================] - 69s 331ms/step - loss: 0.0161 - accuracy: 0.9946 - val_loss: 0.0516 - val_accuracy: 0.9871\n",
      "Epoch 9/10\n",
      "210/210 [==============================] - 65s 310ms/step - loss: 0.0148 - accuracy: 0.9947 - val_loss: 0.0477 - val_accuracy: 0.9869\n",
      "Epoch 10/10\n",
      "210/210 [==============================] - 65s 307ms/step - loss: 0.0116 - accuracy: 0.9959 - val_loss: 0.0455 - val_accuracy: 0.9882\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = 'accuracy')\n",
    "modelpath = './mnist_mymodel/{epoch:003d}-{val_loss:.4f}.h5'\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath = modelpath,\n",
    "                                        save_best_only = True)\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience = 10)\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "history = model.fit(x_train, y_train, epochs = 10, batch_size = 200,\n",
    "                    verbose = 1, validation_split=0.3, \n",
    "                    callbacks = [checkpoint, early_stopping])\n",
    "\n",
    "\n",
    "# history = model.fit(x_train, y_train, validation_data = (x_test, y_test),\n",
    "#                     epochs = 30, batch_size = 200, verbose = 1,\n",
    "#                     callbacks = [early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4065ef09-ecef-4e5c-9303-75f15da7d3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.0325 - accuracy: 0.9909\n",
      "\n",
      " Test accuracy: 0.9909%\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.0325 - accuracy: 0.9909\n",
      "[0.03245822340250015, 0.9908999800682068]\n"
     ]
    }
   ],
   "source": [
    "print('\\n Test accuracy: {:.4f}%'.format(model.evaluate(x_test, y_test)[1]))\n",
    "print(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1590578c-25f2-4dd2-9d7f-a1bef3e2561e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_mnist/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./model_mnist/', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d68e04c8-675e-481f-9159-f56f6ceeb83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf2onnx\n",
      "  Downloading tf2onnx-1.9.3-py3-none-any.whl (435 kB)\n",
      "Collecting onnx>=1.4.1\n",
      "  Downloading onnx-1.11.0-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tf2onnx) (2.27.1)\n",
      "Requirement already satisfied: six in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tf2onnx) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.14.1 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tf2onnx) (1.21.5)\n",
      "Collecting flatbuffers~=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from onnx>=1.4.1->tf2onnx) (4.1.1)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from onnx>=1.4.1->tf2onnx) (3.19.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->tf2onnx) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->tf2onnx) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->tf2onnx) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->tf2onnx) (2021.10.8)\n",
      "Installing collected packages: onnx, flatbuffers, tf2onnx\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "Successfully installed flatbuffers-1.12 onnx-1.11.0 tf2onnx-1.9.3\n"
     ]
    }
   ],
   "source": [
    "# 오닉스 세이브를 위한 설치\n",
    "!pip install -U tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3735dcb-0e32-4796-90fb-665148add241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 15:59:55.867680: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-05-04 15:59:55.867724: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "C:\\Users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-05-04 15:59:58.980653: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-05-04 15:59:58.980695: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-04 15:59:58.984086: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-1O5IRMC\n",
      "2022-05-04 15:59:58.984183: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-1O5IRMC\n",
      "2022-05-04 15:59:58.984523: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-04 15:59:58,985 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-05-04 15:59:59,220 - INFO - Signatures found in model: [serving_default].\n",
      "2022-05-04 15:59:59,220 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-05-04 15:59:59,220 - INFO - Output names: ['dense_5']\n",
      "2022-05-04 15:59:59.221848: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-05-04 15:59:59.222012: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-05-04 15:59:59.236775: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 48 nodes (36), 63 edges (51), time = 4.692ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tf2onnx\\tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-05-04 15:59:59,342 - WARNING - From C:\\Users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tf2onnx\\tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-05-04 15:59:59.348767: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-05-04 15:59:59.348918: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-05-04 15:59:59.392639: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 32 nodes (-16), 47 edges (-16), time = 27.796ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "  constant_folding: Graph size after: 32 nodes (0), 47 edges (0), time = 5.926ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2022-05-04 15:59:59,421 - INFO - Using tensorflow=2.8.0, onnx=1.11.0, tf2onnx=1.9.3/1190aa\n",
      "2022-05-04 15:59:59,421 - INFO - Using opset <onnx, 9>\n",
      "2022-05-04 15:59:59,683 - INFO - Computed 0 values for constant folding\n",
      "2022-05-04 15:59:59,847 - INFO - Optimizing ONNX model\n",
      "2022-05-04 16:00:00,016 - INFO - After optimization: Cast -1 (1->0), Const +1 (9->10), Identity -6 (6->0), Reshape +1 (1->2), Transpose -5 (6->1)\n",
      "2022-05-04 16:00:00,030 - INFO - \n",
      "2022-05-04 16:00:00,031 - INFO - Successfully converted TensorFlow model model_mnist to ONNX\n",
      "2022-05-04 16:00:00,031 - INFO - Model inputs: ['conv2d_4_input']\n",
      "2022-05-04 16:00:00,031 - INFO - Model outputs: ['dense_5']\n",
      "2022-05-04 16:00:00,031 - INFO - ONNX model is saved at model_mnist.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --saved-model model_mnist --output model_mnist.onnx\n",
    "\n",
    "# 설치하고 나서 model_mnist.onnx를 이동한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7350376f-0f44-47ce-818f-925663df81de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 (96.90%)\n",
      "6 (88.28%)\n",
      "9 (66.83%)\n",
      "1 (15.90%)\n",
      "2 (69.08%)\n",
      "3 (96.06%)\n",
      "4 (64.04%)\n",
      "5 (88.65%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    global oldx, oldy\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "        \n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x, y), 255, 10, cv2.LINE_AA)\n",
    "            cv2.imshow('image', img)\n",
    "            oldx, oldy = x, y\n",
    "\n",
    "# 무게 중심 함수\n",
    "def norm_digit(img):\n",
    "    m = cv2.moments(img)\n",
    "    cx = m['m10'] / m['m00']\n",
    "    cy = m['m01'] / m['m00']\n",
    "    h,w = img.shape[:2]\n",
    "    aff = np.array([[1,0, w/2-cx], [0,1,h/2-cy]], dtype = np.float32)\n",
    "    dst = cv2.warpAffine(img, aff, (0,0))\n",
    "    return dst\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNet('./model_mnist.onnx')\n",
    "\n",
    "if net.empty():\n",
    "    print('Net load failed')\n",
    "    sys.exit()\n",
    "    \n",
    "img = np.zeros((400, 400), np.uint8)\n",
    "cv2.imshow('image', img)\n",
    "cv2.setMouseCallback('image', on_mouse)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey()\n",
    "    \n",
    "    if key == 27:\n",
    "        break\n",
    "        \n",
    "    elif key == ord(' '):\n",
    "        \n",
    "        #직접 그린 숫자 400*400짜리 이미지를 blob이라는 객체로 변환\n",
    "        #blob은 4차원 형식의 ndarray (N,C,H,W)\n",
    "        #학습 시, 학습데이터 픽셀값을 0~1까지 정규화하여 학습했으므로 1/255.을 넣어서 똑같이 정규화 해준다\n",
    "        #학습 데이터 mnist 크기그 28*28이였으므로 똑같은 크기인 28*28 resize 시행 \n",
    "        #내가 그린 숫자를 가운데에 위치시켜 놓고 진행: norm_digit\n",
    "        blob = cv2.dnn.blobFromImage(norm_digit(img), 1/255, (28, 28))\n",
    "        net.setInput(blob)\n",
    "        prob = net.forward()\n",
    "        \n",
    "        '''\n",
    "        prob: (1,10) 10개의 elemnet가 있고,\n",
    "        확률이 최대값인 element의 인덱스값 = 모델이 인식한 숫자\n",
    "        maxLoc : 가장 확률이 높은 결과\n",
    "        maxVal : 확률\n",
    "        '''\n",
    "        _, maxVal, _, maxLoc = cv2.minMaxLoc(prob)\n",
    "        \n",
    "        digit = maxLoc[0]\n",
    "        \n",
    "        print(f'{digit} ({maxVal*100:4.2f}%)')\n",
    "        img.fill(0)\n",
    "        cv2.imshow('image', img)\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d02ca2-4a32-4256-9a20-38be79a15028",
   "metadata": {},
   "outputs": [],
   "source": [
    "moveWindow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
