{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch10_seq2seq_나의작업.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "9KqmtiI5LLup"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 한글 자료 \n",
        "raw_inputs = [\n",
        "             \"춤을 추는 쏘세지\"\n",
        "]\n",
        "\n",
        "words = []\n",
        "for s in raw_inputs:\n",
        "    words.extend(s.split()) # extend : s를 추가한다.\n",
        "\n",
        "print(words)\n",
        "\n",
        "## 중복된 단어를 제거\n",
        "words = list(dict.fromkeys(words))\n",
        "print(words)\n",
        "\n",
        "## 단어장의 특수한 토큰을 삽입\n",
        "word_to_id = {\"<PAD>\" : 0, \"<UNK>\" : 1}\n",
        "\n",
        "for w in words:\n",
        "    word_to_id[w] = len(word_to_id)\n",
        "    print(word_to_id)\n",
        "\n",
        "print(word_to_id.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK1K22wYJQBx",
        "outputId": "bdd444e4-5dcd-49c7-f940-63a4a15daa9a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['춤을', '추는', '쏘세지']\n",
            "['춤을', '추는', '쏘세지']\n",
            "{'<PAD>': 0, '<UNK>': 1, '춤을': 2}\n",
            "{'<PAD>': 0, '<UNK>': 1, '춤을': 2, '추는': 3}\n",
            "{'<PAD>': 0, '<UNK>': 1, '춤을': 2, '추는': 3, '쏘세지': 4}\n",
            "dict_items([('<PAD>', 0), ('<UNK>', 1), ('춤을', 2), ('추는', 3), ('쏘세지', 4)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_words = {i : w for w, i in word_to_id.items()}\n",
        "print(id_to_words)\n",
        "\n",
        "train_inputs = []\n",
        "for s in raw_inputs:\n",
        "    row = [word_to_id[w] for w in s.split()]\n",
        "\n",
        "    # padding \n",
        "    row += [0] * (5-len(row))\n",
        "    train_inputs.append(row)\n",
        "\n",
        "train_inputs = np.array(train_inputs)\n",
        "print(train_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM3t35U2LA98",
        "outputId": "1d9f0b33-17f0-45db-8118-95cf9037663f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '<PAD>', 1: '<UNK>', 2: '춤을', 3: '추는', 4: '쏘세지'}\n",
            "[[2 3 4 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot matrix 생성\n",
        "onehot_matrix = np.eye(len(word_to_id))\n",
        "print(onehot_matrix)\n",
        "print(onehot_matrix.shape) # 단어장은 7개인데, 0, 1번째 row는 token을 의미한다.\n",
        "\n",
        "train_onehot = onehot_matrix[train_inputs]\n",
        "print(train_onehot)\n",
        "\n",
        "# one hot coding -> sequence 형태로 복원 가능\n",
        "# 모델에 집어 넣기 전에 squence형태로 복원한다.\n",
        "train_seq = np.argmax(train_onehot, axis = -1)\n",
        "print(train_seq)\n",
        "\n",
        "train_seq_len = train_onehot.shape\n",
        "inp = tf.convert_to_tensor(train_seq, dtype=tf.int32)\n",
        "inp_len = tf.convert_to_tensor(train_seq_len, dtype=tf.int32)\n",
        "\n",
        "\n",
        "print(inp)\n",
        "print(inp_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn0fdQtmLdDC",
        "outputId": "184ae1a6-1f46-4a67-bdf4-76f813ef81b3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1.]]\n",
            "(5, 5)\n",
            "[[[0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 1.]\n",
            "  [1. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0.]]]\n",
            "[[2 3 4 0 0]]\n",
            "tf.Tensor([[2 3 4 0 0]], shape=(1, 5), dtype=int32)\n",
            "tf.Tensor([1 5 5], shape=(3,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 영문 문장 sequence를 만들기 "
      ],
      "metadata": {
        "id": "UgMtAgDDuPEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 한글 자료 \n",
        "raw_inputs = [\n",
        "              \"Dancing Sausage\",\n",
        "              \"Sausage is delicious. How is this sausage made?\",\n",
        "              \"sausage, meat product made of finely chopped and seasoned meat, which may be fresh, smoked, or pickled and which is then usually stuffed into a casing.\"\n",
        "]\n",
        "\n",
        "words = []\n",
        "for s in raw_inputs:\n",
        "    words.extend(s.split()) # extend : s를 추가한다.\n",
        "\n",
        "print(words)\n",
        "\n",
        "## 중복된 단어를 제거\n",
        "words = list(dict.fromkeys(words))\n",
        "print(words)\n",
        "\n",
        "## 단어장의 특수한 토큰을 삽입\n",
        "word_to_id = {\"<PAD>\" : 0, \"<UNK>\" : 1}\n",
        "\n",
        "for w in words:\n",
        "    word_to_id[w] = len(word_to_id)\n",
        "    print(word_to_id)\n",
        "\n",
        "print(word_to_id.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhlcwRbHuOf3",
        "outputId": "69b3153f-80ea-4b8b-c14c-b4ac19e408d3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dancing', 'Sausage', 'Sausage', 'is', 'delicious.', 'How', 'is', 'this', 'sausage', 'made?', 'sausage,', 'meat', 'product', 'made', 'of', 'finely', 'chopped', 'and', 'seasoned', 'meat,', 'which', 'may', 'be', 'fresh,', 'smoked,', 'or', 'pickled', 'and', 'which', 'is', 'then', 'usually', 'stuffed', 'into', 'a', 'casing.']\n",
            "['Dancing', 'Sausage', 'is', 'delicious.', 'How', 'this', 'sausage', 'made?', 'sausage,', 'meat', 'product', 'made', 'of', 'finely', 'chopped', 'and', 'seasoned', 'meat,', 'which', 'may', 'be', 'fresh,', 'smoked,', 'or', 'pickled', 'then', 'usually', 'stuffed', 'into', 'a', 'casing.']\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20, 'may': 21}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20, 'may': 21, 'be': 22}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20, 'may': 21, 'be': 22, 'fresh,': 23}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20, 'may': 21, 'be': 22, 'fresh,': 23, 'smoked,': 24}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20, 'may': 21, 'be': 22, 'fresh,': 23, 'smoked,': 24, 'or': 25}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20, 'may': 21, 'be': 22, 'fresh,': 23, 'smoked,': 24, 'or': 25, 'pickled': 26}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20, 'may': 21, 'be': 22, 'fresh,': 23, 'smoked,': 24, 'or': 25, 'pickled': 26, 'then': 27}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20, 'may': 21, 'be': 22, 'fresh,': 23, 'smoked,': 24, 'or': 25, 'pickled': 26, 'then': 27, 'usually': 28}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20, 'may': 21, 'be': 22, 'fresh,': 23, 'smoked,': 24, 'or': 25, 'pickled': 26, 'then': 27, 'usually': 28, 'stuffed': 29}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20, 'may': 21, 'be': 22, 'fresh,': 23, 'smoked,': 24, 'or': 25, 'pickled': 26, 'then': 27, 'usually': 28, 'stuffed': 29, 'into': 30}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20, 'may': 21, 'be': 22, 'fresh,': 23, 'smoked,': 24, 'or': 25, 'pickled': 26, 'then': 27, 'usually': 28, 'stuffed': 29, 'into': 30, 'a': 31}\n",
            "{'<PAD>': 0, '<UNK>': 1, 'Dancing': 2, 'Sausage': 3, 'is': 4, 'delicious.': 5, 'How': 6, 'this': 7, 'sausage': 8, 'made?': 9, 'sausage,': 10, 'meat': 11, 'product': 12, 'made': 13, 'of': 14, 'finely': 15, 'chopped': 16, 'and': 17, 'seasoned': 18, 'meat,': 19, 'which': 20, 'may': 21, 'be': 22, 'fresh,': 23, 'smoked,': 24, 'or': 25, 'pickled': 26, 'then': 27, 'usually': 28, 'stuffed': 29, 'into': 30, 'a': 31, 'casing.': 32}\n",
            "dict_items([('<PAD>', 0), ('<UNK>', 1), ('Dancing', 2), ('Sausage', 3), ('is', 4), ('delicious.', 5), ('How', 6), ('this', 7), ('sausage', 8), ('made?', 9), ('sausage,', 10), ('meat', 11), ('product', 12), ('made', 13), ('of', 14), ('finely', 15), ('chopped', 16), ('and', 17), ('seasoned', 18), ('meat,', 19), ('which', 20), ('may', 21), ('be', 22), ('fresh,', 23), ('smoked,', 24), ('or', 25), ('pickled', 26), ('then', 27), ('usually', 28), ('stuffed', 29), ('into', 30), ('a', 31), ('casing.', 32)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_words = {i : w for w, i in word_to_id.items()}\n",
        "print(id_to_words)\n",
        "\n",
        "train_inputs = []\n",
        "for s in raw_inputs:\n",
        "    row = [word_to_id[w] for w in s.split()]\n",
        "\n",
        "    # padding \n",
        "    row += [0] * (5-len(row))\n",
        "    train_inputs.append(row)\n",
        "\n",
        "y_train_inputs = np.array(train_inputs)\n",
        "print(\"y_train_inputs : \", y_train_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWxwp-Bzuxo2",
        "outputId": "8ccf63b6-9acd-4d34-f0a6-b175171ee6bc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '<PAD>', 1: '<UNK>', 2: 'Dancing', 3: 'Sausage', 4: 'is', 5: 'delicious.', 6: 'How', 7: 'this', 8: 'sausage', 9: 'made?', 10: 'sausage,', 11: 'meat', 12: 'product', 13: 'made', 14: 'of', 15: 'finely', 16: 'chopped', 17: 'and', 18: 'seasoned', 19: 'meat,', 20: 'which', 21: 'may', 22: 'be', 23: 'fresh,', 24: 'smoked,', 25: 'or', 26: 'pickled', 27: 'then', 28: 'usually', 29: 'stuffed', 30: 'into', 31: 'a', 32: 'casing.'}\n",
            "y_train_inputs :  [list([2, 3, 0, 0, 0]) list([3, 4, 5, 6, 4, 7, 8, 9])\n",
            " list([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 17, 20, 4, 27, 28, 29, 30, 31, 32])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# seq2seq 모델"
      ],
      "metadata": {
        "id": "q0MlIIp8wfZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://aiffelstaticprd.blob.core.windows.net/media/images/GN-4-L-7.max-800x600.jpg)"
      ],
      "metadata": {
        "id": "BYf-_1t1wiII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder 구현"
      ],
      "metadata": {
        "id": "SUO9UmWfwk2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://aiffelstaticprd.blob.core.windows.net/media/images/GN-4-L-6.max-800x600.jpg)"
      ],
      "metadata": {
        "id": "lCPSBCPdwnNf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "aT83VDAopzxr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units): # enc_units :encode hidden unit\n",
        "        super(Encoder, self).__init__()\n",
        "        # Embedding의 인자는 외우면 좋다. 이해하면 더 좋다.\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # 나는 밥을 먹었어\n",
        "        self.lstm = tf.keras.layers.LSTM(enc_units) # enc_units :encode hidden unit\n",
        "    \n",
        "    def call(self, x):     # x는 sequence 형태(숫자 형태)로 넣어야 한다.\n",
        "        print(\"입력 shape :\", x.shape) # sample input // 춤 추는 소시지 (1, 3)\n",
        "        print(\"입력 값 :\", x)\n",
        "        # Embedding Layer를 거친 shape : (1, 3, 256) 한 문장의 3개 단어, 단어당 256개의 vector가 존재함\n",
        "        x = self.embedding(x) # ex) (0.699, 0.92, 0.89) = self.embedding((0, 1, 2))\n",
        "        print(\"Embedding Layer를 거친 shape :\", x.shape)\n",
        "\n",
        "        output = self.lstm(x)\n",
        "        print(\"LSTM shape의 output shape :\", output.shape)\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "slWrhuY1wuw5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 30000\n",
        "emb_size = 256\n",
        "lstm_size = 512\n",
        "batch_size = 1\n",
        "sample_seq_len = 3 # 춤 추는 소시지가 3 단어이기 때문에 값이 3\n",
        "\n",
        "print(\"Vocab Size : {0}\".format(vocab_size))\n",
        "print(\"Embedding Size : {0}\".format(emb_size))\n",
        "print(\"LSTM Size : {0}\".format(lstm_size))\n",
        "print(\"Batch_size : {0}\".format(batch_size))\n",
        "print(\"Sample Sequence Length : {0}\".format(sample_seq_len))"
      ],
      "metadata": {
        "id": "2mgOnbTXxCMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24eaec7d-4ad6-4d04-e613-d835a976c0c1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size : 30000\n",
            "Embedding Size : 256\n",
            "LSTM Size : 512\n",
            "Batch_size : 1\n",
            "Sample Sequence Length : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_size, emb_size, lstm_size) #vocab_size, embedding_dim, enc_units):\n",
        "# 테스트할 때는 sample_input = (0, 0, 0)으로 초기화로 값을 넣다.\n",
        "sample_input = tf.zeros((batch_size, sample_seq_len)) # 춤 추는 소시지\n",
        "\n",
        "\n",
        "sample_output_encoder = encoder(sample_input)\n",
        "sample_output_encoder_test = encoder(inp) # 안되는 걸 보니 평문이 아닌 one hot코딩?"
      ],
      "metadata": {
        "id": "T0nOgRrexIzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8eae30d-3094-4829-aada-0ac462680cd6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 shape : (1, 3)\n",
            "입력 값 : tf.Tensor([[0. 0. 0.]], shape=(1, 3), dtype=float32)\n",
            "Embedding Layer를 거친 shape : (1, 3, 256)\n",
            "LSTM shape의 output shape : (1, 512)\n",
            "입력 shape : (1, 5)\n",
            "입력 값 : tf.Tensor([[2 3 4 0 0]], shape=(1, 5), dtype=int32)\n",
            "Embedding Layer를 거친 shape : (1, 5, 256)\n",
            "LSTM shape의 output shape : (1, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder 구현"
      ],
      "metadata": {
        "id": "YaCFAU30xPMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://aiffelstaticprd.blob.core.windows.net/media/images/GN-4-L-7.max-800x600.jpg)"
      ],
      "metadata": {
        "id": "dFRBUyamxRYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # return_sequences=True 이전 state의 정보를 각각 다 출력한다는 의미\n",
        "        # False면 <END> 토큰만 나온다. True면 dancing, Sausage 도 출력이 된다.\n",
        "        self.lstm = tf.keras.layers.LSTM(dec_units, return_sequences=True) ####\n",
        "        \n",
        "        # Full connected Layer = Dense = mlp(multi layer perceptron) 결국 같은 말이다.\n",
        "        # Full connected Layer : 현재 층의 모든 뉴런이 완전히 다음 층의 모든 뉴런과 연결되었다\n",
        "        # 꼭 relu 아니어도 되고, activation이 없어도 된다.\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size,  activation='relu') # full connect Layer\n",
        "        # self.softmax = tf.keras.layers.Dense(dec_units, activation='softmax')# None (변수 5)\n",
        "        self.softmax = tf.keras.layers.Softmax(axis= -1 )\n",
        "\n",
        "    def call(self, x, context_v):\n",
        "        print(\"입력 shape : \", x.shape)\n",
        "        print(\"입력 값 :\", x)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        print(\"Embedding layer를 거친 shape\", x.shape)\n",
        "\n",
        "        # expand_dims : 행렬을 추가해준다.\n",
        "        # 행렬을 늘려서 x와 context_v가 행렬 계산이 가능하도록 shape를 맞춰준다.\n",
        "        # tf.expand_dims(context_v, axis=1)은 (1, 512)에서 512(1축)를 의미한다.\n",
        "        context_v = tf.repeat(tf.expand_dims(context_v, axis=1), repeats=x.shape[1], axis=1)\n",
        "        x = tf.concat([x, context_v], axis = -1) # [변수 1, 변수 2], axis = -1)\n",
        "        print('Context Vector가 더해진 shape : ', x.shape)\n",
        "\n",
        "        x = self.lstm(x)\n",
        "        print(\"LSTM layer의 output layer : \", x.shape)\n",
        "\n",
        "        output = self.fc(x)\n",
        "        print(\"Decoder의 최종 ouput layer : \", output.shape)\n",
        "\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "QNPaQgSVxNvG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 30000\n",
        "emb_size = 256\n",
        "lstm_size = 512\n",
        "batch_size = 1\n",
        "sample_seq_len = 3\n",
        "\n",
        "print(\"Vocab Size : {0}\".format(vocab_size))\n",
        "print(\"Embedding Size : {0}\".format(emb_size))\n",
        "print(\"LSTM Size : {0}\".format(lstm_size))\n",
        "print(\"Batch_size : {0}\".format(batch_size))\n",
        "print(\"Sample Sequence Length : {0}\".format(sample_seq_len))"
      ],
      "metadata": {
        "id": "zmLtmubHxzyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e96662-86b4-447f-ec69-5795d4306793"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size : 30000\n",
            "Embedding Size : 256\n",
            "LSTM Size : 512\n",
            "Batch_size : 1\n",
            "Sample Sequence Length : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_size, emb_size, lstm_size)\n",
        "sample_input = tf.zeros((batch_size, sample_seq_len)) # (1, 3) 행렬이 0으로 채워진다.\n",
        "\n",
        "sample_output_decoder = decoder(sample_input, sample_output_encoder) # call 함수 호출\n",
        "\n",
        "print(sample_output_decoder)\n",
        "\n",
        "# 춤을 추는 쏘세지는 어떻게 나올까?\n",
        "# decoder = Decoder(vocab_size, emb_size, lstm_size)\n",
        "# sample_input_test_eng= y_train_inputs # 이 부분을 영문에 해당되는 sequence를 넣어준다. 즉 y_train 값이다.\n",
        "\n",
        "# sample_output_decoder_eng = decoder(sample_input_test_eng, sample_output_encoder_test)\n",
        "\n",
        "# print(sample_output_decoder_eng)"
      ],
      "metadata": {
        "id": "ZwalJJzKx0W-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35eeff9c-b724-4d47-87da-da95dbe1c983"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 shape :  (1, 3)\n",
            "입력 값 : tf.Tensor([[0. 0. 0.]], shape=(1, 3), dtype=float32)\n",
            "Embedding layer를 거친 shape (1, 3, 256)\n",
            "Context Vector가 더해진 shape :  (1, 3, 768)\n",
            "LSTM layer의 output layer :  (1, 3, 512)\n",
            "Decoder의 최종 ouput layer :  (1, 3, 30000)\n",
            "tf.Tensor(\n",
            "[[[0.00195047 0.0019539  0.0019531  ... 0.00195506 0.00195302 0.00195321]\n",
            "  [0.00194853 0.00195467 0.00195299 ... 0.00195646 0.00195285 0.00195318]\n",
            "  [0.00194712 0.00195546 0.00195279 ... 0.00195754 0.00195274 0.00195317]]], shape=(1, 3, 512), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XuyK_JSlNHpN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention"
      ],
      "metadata": {
        "id": "I2rWe11Hyake"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bahdanau Attention\n",
        "$$ Score_{alignment} = W * tanh(W_{decoder} * H_{decoder} + W_{encoder} * H_{encoder}) $$"
      ],
      "metadata": {
        "id": "HVVLgiLHycqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "W2CJ2qliyaxn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.w_decoder = tf.keras.layers.Dense(units)\n",
        "        self.w_encoder = tf.keras.layers.Dense(units)\n",
        "        self.w_combine = tf.keras.layers.Dense(1) # 값을 합치기 때문에 1로 설정한다.\n",
        "    \n",
        "    def call(self, h_encoder, h_decoder):\n",
        "        print(\"[H_encoder] shape : \", h_encoder.shape)\n",
        "\n",
        "        h_encoder = self.w_encoder(h_encoder)\n",
        "        print(\"[w_encoder x h_encoder] shape : \", h_encoder.shape)\n",
        "\n",
        "        print(\"\\n[H_decoder shape :\", h_decoder.shape)\n",
        "        h_decoder = tf.expand_dims(h_decoder, 1)\n",
        "        h_decoder = self.w_decoder(h_decoder)\n",
        "\n",
        "        print(\"[w_encoder x h_decoder] shape :\", h_decoder.shape)\n",
        "\n",
        "        score = self.w_combine(tf.nn.tanh(h_decoder + h_encoder))\n",
        "        print(\"[score alinment] shape :\", score.shape)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        print(\"\\n최종 weight : \\n\", attention_weights.numpy())\n",
        "\n",
        "        context_vector = attention_weights * h_decoder\n",
        "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "9csARr_vyftu"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_size = 100\n",
        "print(\"hidden state {0}차원으로 Mapping \\n\".format(w_size))\n",
        "\n",
        "attention = BahdanauAttention(w_size)\n",
        "\n",
        "enc_state = tf.random.uniform((1, 10, 512))\n",
        "dec_state = tf.random.uniform((1, 512))\n",
        "\n",
        "_ = attention(enc_state, dec_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiaYBKa6RCdw",
        "outputId": "fbbd1144-13b8-4831-ffe4-b8ac99f9dbe3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden state 100차원으로 Mapping \n",
            "\n",
            "[H_encoder] shape :  (1, 10, 512)\n",
            "[w_encoder x h_encoder] shape :  (1, 10, 100)\n",
            "\n",
            "[H_decoder shape : (1, 512)\n",
            "[w_encoder x h_decoder] shape : (1, 1, 100)\n",
            "[score alinment] shape : (1, 10, 1)\n",
            "\n",
            "최종 weight : \n",
            " [[[0.15979587]\n",
            "  [0.07183252]\n",
            "  [0.12006551]\n",
            "  [0.05205782]\n",
            "  [0.06597082]\n",
            "  [0.07402176]\n",
            "  [0.12115809]\n",
            "  [0.07401933]\n",
            "  [0.125352  ]\n",
            "  [0.13572636]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PDUVChYvRDl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loung Attention"
      ],
      "metadata": {
        "id": "eSfQTfeGzbLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ Score(H_{target},H_{source}) = H_{target}^T * W_{combine} * H_{source}$$"
      ],
      "metadata": {
        "id": "gu8BHyqmzc2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.w_combine = tf.keras.layers.Dense(units)\n",
        "\n",
        "    def call(self, h_encoder, h_decoder):\n",
        "        print(\"[h_encoder] shape : \", h_encoder.shape)\n",
        "\n",
        "        wh = self.w_combine(h_encoder)\n",
        "        print(\"[W_encoder x h_encoder] shape :\", wh.shape)\n",
        "\n",
        "        h_decoder = tf.expand_dims(h_decoder, 1)\n",
        "        alignment = tf.matmul(wh, tf.transpose(h_decoder, [0, 2, 1]))\n",
        "        print(\"[Score alignment] shape : \", alignment.shape)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(alignment, axis=1)\n",
        "        print(\"\\n 최종 weight : \\n\", attention_weights.numpy())\n",
        "\n",
        "        # 다시 행렬 차원을 줄인다.\n",
        "        attention_weights = tf.squeeze(attention_weights, axis= -1)\n",
        "        context_vector = tf.matmul(attention_weights, h_encoder)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "UO1Xn8V5zbXt"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 512\n",
        "attention = LuongAttention(emb_size)\n",
        "\n",
        "enc_state = tf.random.uniform((1, 10, emb_size))\n",
        "dec_state = tf.random.uniform((1, emb_size))\n",
        "\n",
        "_ = attention(enc_state, dec_state)"
      ],
      "metadata": {
        "id": "qd-KxPyKzo5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73238e5-0eae-40a1-b9b8-df5b6bc91e7a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[h_encoder] shape :  (1, 10, 256)\n",
            "[W_encoder x h_encoder] shape : (1, 10, 256)\n",
            "[Score alignment] shape :  (1, 10, 1)\n",
            "\n",
            " 최종 weight : \n",
            " [[[1.4172524e-01]\n",
            "  [1.3149093e-03]\n",
            "  [3.8371602e-04]\n",
            "  [6.6584784e-05]\n",
            "  [2.6182837e-03]\n",
            "  [9.1909263e-03]\n",
            "  [4.0929201e-03]\n",
            "  [3.9876853e-03]\n",
            "  [5.4108638e-01]\n",
            "  [2.9553339e-01]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax"
      ],
      "metadata": {
        "id": "sUX7GVEiUnHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "4XV4bwJGUjE1"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()"
      ],
      "metadata": {
        "id": "nG_TsIQcU1aq"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_logit = np.array([-2, 1.5, -1, 0.5, 2])\n",
        "pred_prob = softmax(pred_logit)"
      ],
      "metadata": {
        "id": "VQOXhYawU8uZ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = [18, 3])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('predicted logit')\n",
        "plt.bar(np.arange(len(pred_logit)), pred_logit)\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('predicted probility')\n",
        "plt.bar(np.arange(len(pred_prob)), pred_prob)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "uwHGPYxTVHXI",
        "outputId": "cecefad6-134d-4eac-b3a2-873d5172caeb"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x216 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "COt1Vqf1Vm99"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}